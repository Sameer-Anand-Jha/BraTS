{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f87d1a-1006-4af2-95b8-80b590ec086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell: Dataset for BraTS 2D slice segmentation\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "brats_dir = \"/home/hiranmoy/Downloads/Sameer/Brats/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "\n",
    "class BraTSSliceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for 2D slice segmentation from BraTS. Binary segmentation: tumor vs background.\n",
    "    Modalities: flair, t1, t1ce, t2. \n",
    "    modality_mode: \n",
    "       'all'      -> use all 4 modalities stacked as 4-channel input,\n",
    "       or a list like ['flair'], ['t1ce','flair'] for simultaneous subset.\n",
    "    filter_empty: if True, exclude slices where mask is all-zero.\n",
    "    transforms: a function that takes (image: np.ndarray shape [C,H,W], mask: np.ndarray [H,W]) and returns augmented versions.\n",
    "    \"\"\"\n",
    "    def __init__(self, brats_dir, patient_list=None, modality_mode='all', filter_empty=True, transforms=None):\n",
    "        \"\"\"\n",
    "        brats_dir: root directory containing BraTS20_Training_xxx folders.\n",
    "        patient_list: list of folder names or full paths to include; if None, list all.\n",
    "        modality_mode: 'all' or list of modality strings among ['flair','t1','t1ce','t2'].\n",
    "        filter_empty: whether to drop slices where mask has zero tumor pixels.\n",
    "        transforms: callable(image, mask) -> (image, mask), for data augmentation.\n",
    "        \"\"\"\n",
    "        self.brats_dir = brats_dir\n",
    "        # Determine modalities to load\n",
    "        all_mods = ['flair','t1','t1ce','t2']\n",
    "        if modality_mode == 'all':\n",
    "            self.modalities = all_mods\n",
    "        else:\n",
    "            # ensure list\n",
    "            assert isinstance(modality_mode, (list,tuple)), \"modality_mode must be 'all' or list\"\n",
    "            for m in modality_mode:\n",
    "                assert m in all_mods, f\"Unknown modality {m}\"\n",
    "            self.modalities = modality_mode\n",
    "        \n",
    "        # List patients\n",
    "        if patient_list is None:\n",
    "            # list all directories starting with 'BraTS20_Training'\n",
    "            dirs = sorted([d for d in os.listdir(brats_dir) \n",
    "                           if os.path.isdir(os.path.join(brats_dir,d)) and d.startswith(\"BraTS20_Training\")])\n",
    "            self.patients = [os.path.join(brats_dir, d) for d in dirs]\n",
    "        else:\n",
    "            # user-provided list of full paths or folder names\n",
    "            tmp = []\n",
    "            for p in patient_list:\n",
    "                # if just folder name:\n",
    "                if os.path.basename(p).startswith(\"BraTS20_Training\") and not os.path.isabs(p):\n",
    "                    tmp.append(os.path.join(brats_dir, p))\n",
    "                else:\n",
    "                    tmp.append(p)\n",
    "            self.patients = tmp\n",
    "        \n",
    "        self.filter_empty = filter_empty\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Build index: list of (patient_path, slice_idx)\n",
    "        self.index = []\n",
    "        for p in self.patients:\n",
    "            # Load mask once to know shape\n",
    "            mask_nii = nib.load(os.path.join(p, os.path.basename(p) + '_seg.nii'))\n",
    "            mask_vol = mask_nii.get_fdata()  # shape [H, W, D]\n",
    "            _, _, D = mask_vol.shape\n",
    "            for z in range(D):\n",
    "                if filter_empty:\n",
    "                    sl = mask_vol[..., z]\n",
    "                    if np.all(sl == 0):\n",
    "                        continue\n",
    "                self.index.append((p, z))\n",
    "        print(f\"BraTSSliceDataset: {len(self.index)} slices (from {len(self.patients)} patients), modalities={self.modalities}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p, z = self.index[idx]\n",
    "        base = os.path.basename(p)\n",
    "        # Load modalities for slice z\n",
    "        imgs = []\n",
    "        for m in self.modalities:\n",
    "            nii_path = os.path.join(p, f\"{base}_{m}.nii\")\n",
    "            arr = nib.load(nii_path).get_fdata()  # [H, W, D]\n",
    "            sl = arr[..., z]  # [H, W]\n",
    "            # Normalize per-slice: z-score\n",
    "            mu, sd = sl.mean(), sl.std()\n",
    "            if sd > 0:\n",
    "                sl = (sl - mu) / sd\n",
    "            else:\n",
    "                sl = sl - mu\n",
    "            imgs.append(sl.astype(np.float32))\n",
    "        image = np.stack(imgs, axis=0)  # [C, H, W]\n",
    "        # Load mask slice, binary\n",
    "        mask_nii = nib.load(os.path.join(p, f\"{base}_seg.nii\"))\n",
    "        mask_vol = mask_nii.get_fdata()\n",
    "        msl = mask_vol[..., z]\n",
    "        # Binary: tumor if label>0\n",
    "        mask = (msl > 0).astype(np.float32)  # [H, W]\n",
    "        \n",
    "        # Apply transforms if any (e.g. random flip/rotate). They should handle image [C,H,W] and mask [H,W]\n",
    "        if self.transforms is not None:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "        \n",
    "        # To tensor\n",
    "        image_t = torch.from_numpy(image)  # [C,H,W]\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0)  # [1,H,W], as channel for BCE or similar\n",
    "        return image_t, mask_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fd52f3-b552-45d7-aad7-097856cc9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def random_flip_rotate(image, mask):\n",
    "    # image: np [C,H,W], mask: np [H,W]\n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = image[:, :, ::-1]\n",
    "        mask = mask[:, ::-1]\n",
    "    # Random vertical flip\n",
    "    if random.random() < 0.5:\n",
    "        image = image[:, ::-1, :]\n",
    "        mask = mask[:, ::-1]\n",
    "    # Random rotation by 90 deg multiples\n",
    "    k = random.choice([0,1,2,3])\n",
    "    if k>0:\n",
    "        image = np.rot90(image, k, axes=(1,2))\n",
    "        mask = np.rot90(mask, k, axes=(0,1))\n",
    "    return image.copy(), mask.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39ead6a-2fd8-429e-bf9a-426cc3bb69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects found: 369\n",
      "Train subjects: 258, Val subjects: 111\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Cell: Subject-wise split for BraTS 2D experiments\n",
    "import os, random\n",
    "\n",
    "# Root directory containing BraTS patient folders\n",
    "brats_dir = \"/home/hiranmoy/Downloads/Sameer/Brats/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "\n",
    "# List all patient directories (folder names start with \"BraTS20_Training\")\n",
    "all_dirs = sorted([d for d in os.listdir(brats_dir)\n",
    "                   if os.path.isdir(os.path.join(brats_dir, d)) and d.startswith(\"BraTS20_Training\")])\n",
    "all_patients = [os.path.join(brats_dir, d) for d in all_dirs]\n",
    "print(\"Total subjects found:\", len(all_patients))  # e.g. ~369\n",
    "\n",
    "# Shuffle and split 70% train / 30% val\n",
    "random.seed(42)\n",
    "random.shuffle(all_patients)\n",
    "split_idx = int(0.7 * len(all_patients))\n",
    "train_subjects = all_patients[:split_idx]\n",
    "val_subjects   = all_patients[split_idx:]\n",
    "print(f\"Train subjects: {len(train_subjects)}, Val subjects: {len(val_subjects)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e9231a-57df-4ce2-8d73-76ddc62cdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: U-Net definition (2D) - from previous\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"(Conv -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "class Down(nn.Sequential):\n",
    "    \"\"\"Downscale: MaxPool then DoubleConv.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsample then DoubleConv, concatenating with skip feature (or zeros).\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_feat=None):\n",
    "        x = self.up(x)\n",
    "        if skip_feat is not None:\n",
    "            # center-crop skip if needed\n",
    "            if x.size(2) != skip_feat.size(2) or x.size(3) != skip_feat.size(3):\n",
    "                diffY = skip_feat.size(2) - x.size(2)\n",
    "                diffX = skip_feat.size(3) - x.size(3)\n",
    "                skip_feat = skip_feat[:, :,\n",
    "                                      diffY//2 : diffY//2 + x.size(2),\n",
    "                                      diffX//2 : diffX//2 + x.size(3)]\n",
    "            x = torch.cat([x, skip_feat], dim=1)\n",
    "        else:\n",
    "            zeros = torch.zeros_like(x)\n",
    "            x = torch.cat([x, zeros], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=1):\n",
    "        super(UNet2D, self).__init__()\n",
    "        self.conv1 = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        self.up3 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up1 = Up(256, 128)\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, in_channels, H, W]\n",
    "        c1 = self.conv1(x)        # [B,64,H,W]\n",
    "        d1 = self.down1(c1)       # [B,128,H/2,W/2]\n",
    "        d2 = self.down2(d1)       # [B,256,H/4,W/4]\n",
    "        d3 = self.down3(d2)       # [B,512,H/8,W/8]\n",
    "        b  = self.bottleneck(d3)  # [B,1024,H/8,W/8]\n",
    "        u3 = self.up3(b, d2)      # [B,512,H/4,W/4]\n",
    "        u2 = self.up2(u3, d1)     # [B,256,H/2,W/2]\n",
    "        u1 = self.up1(u2, c1)     # [B,128,H,W]\n",
    "        out = self.conv_out(u1)   # [B,1,H,W]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44087a44-0176-4e87-9a2a-f76c9a64e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Simplified SegNet for 2D binary segmentation\n",
    "# We implement an encoder-decoder with pooling indices.\n",
    "\n",
    "class SegNet2D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=1):\n",
    "        super(SegNet2D, self).__init__()\n",
    "        # Encoder layers: list of conv blocks; store indices\n",
    "        # For brevity, define 3 encoder stages (like U-Net depth 3)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Bottleneck conv\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Decoder: unpool + conv\n",
    "        self.unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Final conv\n",
    "        self.final_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder stage 1\n",
    "        e1 = self.enc1(x)                  # [B,64,H,W]\n",
    "        p1, idx1 = self.pool1(e1)         # [B,64,H/2,W/2]\n",
    "        # Encoder stage 2\n",
    "        e2 = self.enc2(p1)                # [B,128,H/2,W/2]\n",
    "        p2, idx2 = self.pool2(e2)         # [B,128,H/4,W/4]\n",
    "        # Encoder stage 3\n",
    "        e3 = self.enc3(p2)                # [B,256,H/4,W/4]\n",
    "        p3, idx3 = self.pool3(e3)         # [B,256,H/8,W/8]\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p3)           # [B,512,H/8,W/8]\n",
    "        # Decoder stage 3\n",
    "        up3 = self.unpool3(b, idx3, output_size=e3.size())  # [B,512,H/4,W/4]\n",
    "        d3 = self.dec3(up3)               # [B,256,H/4,W/4]\n",
    "        # Decoder stage 2\n",
    "        up2 = self.unpool2(d3, idx2, output_size=e2.size()) # [B,256,H/2,W/2]\n",
    "        d2 = self.dec2(up2)               # [B,128,H/2,W/2]\n",
    "        # Decoder stage 1\n",
    "        up1 = self.unpool1(d2, idx1, output_size=e1.size()) # [B,128,H,W]\n",
    "        d1 = self.dec1(up1)               # [B,64,H,W]\n",
    "        out = self.final_conv(d1)         # [B,1,H,W]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10bc93e-cfb3-425e-871a-07d5331ad015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE Loss\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, target):\n",
    "        # logits: [B,1,H,W], target: [B,1,H,W], values {0,1}\n",
    "        probs = torch.sigmoid(logits)\n",
    "        B = target.size(0)\n",
    "        probs_flat = probs.view(B, -1)\n",
    "        target_flat = target.view(B, -1)\n",
    "        intersection = (probs_flat * target_flat).sum(dim=1)\n",
    "        union = probs_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
    "        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n",
    "        loss = 1 - dice\n",
    "        return loss.mean()\n",
    "\n",
    "# Generalized Dice Loss (accounting for class imbalance via weights):\n",
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, target):\n",
    "        # Here binary: treat background vs tumor; weight inversely by volume\n",
    "        probs = torch.sigmoid(logits)\n",
    "        B = target.size(0)\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            p = probs[i].view(-1)\n",
    "            g = target[i].view(-1)\n",
    "            # weights: w_c = 1 / (sum(g==c)^2) but binary: two classes\n",
    "            # compute for class 1 (tumor) and class 0 (bg)\n",
    "            # sum over target\n",
    "            # ground truth volumes\n",
    "            vol_pos = g.sum()\n",
    "            vol_neg = (1 - g).sum()\n",
    "            w_pos = 1.0 / (vol_pos * vol_pos + self.smooth)\n",
    "            w_neg = 1.0 / (vol_neg * vol_neg + self.smooth)\n",
    "            # dice for each class\n",
    "            # for tumor class:\n",
    "            inter_pos = (p * g).sum()\n",
    "            dice_pos = (2 * inter_pos + self.smooth) / (p.sum() + g.sum() + self.smooth)\n",
    "            # for background:\n",
    "            p_neg = 1 - p\n",
    "            g_neg = 1 - g\n",
    "            inter_neg = (p_neg * g_neg).sum()\n",
    "            dice_neg = (2 * inter_neg + self.smooth) / (p_neg.sum() + g_neg.sum() + self.smooth)\n",
    "            # weighted sum\n",
    "            loss_i = 1 - (w_pos * dice_pos + w_neg * dice_neg) / (w_pos + w_neg)\n",
    "            losses.append(loss_i)\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "# Focal Loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        # logits: [B,1,H,W], target: [B,1,H,W]\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = torch.where(target == 1, probs, 1 - probs)  # [B,1,H,W]\n",
    "        w = self.alpha * (1 - pt) ** self.gamma\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, target, reduction='none')\n",
    "        loss = w * bce\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "# Tversky Loss \n",
    "\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, smooth=1e-5):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, target):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        B = target.size(0)\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            p = probs[i].view(-1)\n",
    "            g = target[i].view(-1)\n",
    "            tp = (p * g).sum()\n",
    "            fn = ((1 - p) * g).sum()\n",
    "            fp = (p * (1 - g)).sum()\n",
    "            tversky = (tp + self.smooth) / (tp + self.alpha * fn + self.beta * fp + self.smooth)\n",
    "            losses.append(1 - tversky)\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "# Focal Traversky Loss \n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, gamma=1.0, smooth=1e-5):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, target):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        B = target.size(0)\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            p = probs[i].view(-1)\n",
    "            g = target[i].view(-1)\n",
    "            tp = (p * g).sum()\n",
    "            fn = ((1 - p) * g).sum()\n",
    "            fp = (p * (1 - g)).sum()\n",
    "            tversky = (tp + self.smooth) / (tp + self.alpha * fn + self.beta * fp + self.smooth)\n",
    "            loss_i = (1 - tversky) ** self.gamma\n",
    "            losses.append(loss_i)\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "# Robust Dice Loss \n",
    "\n",
    "class RobustDiceLoss(nn.Module):\n",
    "    def __init__(self, lam=2.0, smooth=1e-5):\n",
    "        super(RobustDiceLoss, self).__init__()\n",
    "        self.lam = lam\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, target):\n",
    "        # p^lam in numerator and denom\n",
    "        probs = torch.sigmoid(logits)\n",
    "        B = target.size(0)\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            p = probs[i].view(-1)\n",
    "            g = target[i].view(-1)\n",
    "            p_l = p ** self.lam\n",
    "            inter = (p_l * g).sum()\n",
    "            denom = (p_l * p_l).sum() + (g * g).sum()  # note: original formula uses p^lam squared? or p^(2*lam)? In paper: 2∑ p^λ g / (∑ p^{2λ} + ∑g^2). We'll use p_l^2 = p^(2λ).\n",
    "            # Actually the formula given: Robust Dice = 1 - (2 ∑ p^λ g) / (∑ p^{2λ} + ∑ g^2)\n",
    "            denom = (p ** (2*self.lam)).sum() + (g * g).sum()\n",
    "            dice = (2 * inter + self.smooth) / (denom + self.smooth)\n",
    "            losses.append(1 - dice)\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "# Adaptive Robust Loss \n",
    "\n",
    "class AdaptiveRobustLoss(nn.Module):\n",
    "    def __init__(self, alpha=2.0, c=1.0, reduction='mean'):\n",
    "        super(AdaptiveRobustLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.c = c\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        # Here, input x = difference between prediction probability and target? \n",
    "        # Alternatively, apply robust loss on residual p - g.\n",
    "        probs = torch.sigmoid(logits)\n",
    "        # residual\n",
    "        x = probs - target  # [B,1,H,W]\n",
    "        # compute AR loss elementwise\n",
    "        # AR Loss(x, alpha, c) = |alpha-2|/alpha * (( (x/c)^2 / |alpha-2| + 1)^(alpha/2) - 1)\n",
    "        # but careful with alpha=2: return 0? Might fallback to L2.\n",
    "        eps = 1e-6\n",
    "        a = self.alpha\n",
    "        c = self.c\n",
    "        term = (x / c) ** 2\n",
    "        if abs(a - 2.0) < eps:\n",
    "            # L2 loss scaled\n",
    "            loss_elem = 0.5 * term\n",
    "        else:\n",
    "            loss_elem = torch.abs(a - 2.0)/a * ( (term / torch.abs(a - 2.0) + 1.0).pow(a/2.0) - 1.0 )\n",
    "        if self.reduction == 'mean':\n",
    "            return loss_elem.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss_elem.sum()\n",
    "        else:\n",
    "            return loss_elem\n",
    "# Boundary Loss \n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "    def forward(self, logits, target):\n",
    "        # Compute distance map of ground truth boundary\n",
    "        # This is expensive per batch; precompute distance transforms for each mask in dataset?\n",
    "        # For demonstration: assume single batch size 1.\n",
    "        probs = torch.sigmoid(logits)  # [B,1,H,W]\n",
    "        total_loss = 0.0\n",
    "        B = target.size(0)\n",
    "        for i in range(B):\n",
    "            g = target[i,0].cpu().numpy().astype(np.uint8)  # [H,W]\n",
    "            # Compute distance transform: distance to nearest boundary pixel\n",
    "            # boundary: edges of mask\n",
    "            boundary = np.logical_xor(g, ndi.binary_erosion(g)).astype(np.uint8)\n",
    "            # distance: for each pixel, distance to nearest boundary pixel\n",
    "            # distance transform of inverted boundary: zero at boundary, increases away\n",
    "            dist_map = ndi.distance_transform_edt(1 - boundary)\n",
    "            dist_map = torch.from_numpy(dist_map).to(logits.device).float()  # [H,W]\n",
    "            p = probs[i,0]\n",
    "            # Boundary loss: sum |p - g| * dist_map\n",
    "            total_loss += torch.mean((p - torch.from_numpy(g).to(logits.device).float()).abs() * dist_map)\n",
    "        return total_loss / B\n",
    "\n",
    "# Hausdorff Loss\n",
    "\n",
    "class HausdorffLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HausdorffLoss, self).__init__()\n",
    "        # Possibly precompute distance transforms of gt for entire dataset if memory allows.\n",
    "    def forward(self, logits, target):\n",
    "        # Similar to boundary loss but penalize max distance. Use a surrogate: \n",
    "        # For each pixel in predicted boundary, measure distance to GT boundary via precomputed map.\n",
    "        # Here we define a simple surrogate: mean distance over predicted edges.\n",
    "        probs = torch.sigmoid(logits)\n",
    "        total = 0.0\n",
    "        B = target.size(0)\n",
    "        for i in range(B):\n",
    "            p_np = (probs[i,0].cpu().detach().numpy() > 0.5).astype(np.uint8)\n",
    "            g_np = target[i,0].cpu().numpy().astype(np.uint8)\n",
    "            # boundaries\n",
    "            p_b = np.logical_xor(p_np, ndi.binary_erosion(p_np)).astype(np.uint8)\n",
    "            g_b = np.logical_xor(g_np, ndi.binary_erosion(g_np)).astype(np.uint8)\n",
    "            # distance transform of g boundary\n",
    "            dist_map = ndi.distance_transform_edt(1 - g_b)\n",
    "            # For each predicted boundary pixel, get its distance\n",
    "            if p_b.sum() > 0:\n",
    "                d = dist_map[p_b==1]\n",
    "                total += d.mean()\n",
    "            else:\n",
    "                # if no predicted boundary, penalize heavily\n",
    "                total += dist_map.max()\n",
    "        return torch.tensor(total / B, device=logits.device)\n",
    "\n",
    "\n",
    "\n",
    "# Perceptual Loss \n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.fe = feature_extractor.eval()  # freeze\n",
    "        for p in self.fe.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.l1 = nn.L1Loss()\n",
    "    def forward(self, logits, target, input_image):\n",
    "        \"\"\"\n",
    "        logits: [B,1,H,W], target: [B,1,H,W], input_image: [B,1,H,W] or multi-channel\n",
    "        We compute perceptual loss between predicted mask (or masked image) and ground truth mask?\n",
    "        One approach: treat predicted mask and GT mask as single-channel images, feed through fe.\n",
    "        \"\"\"\n",
    "        pred = torch.sigmoid(logits)\n",
    "        # expand to 3 channels if fe expects 3?\n",
    "        # For simplicity, tile to 3 channels or adapt feature_extractor to 1-channel.\n",
    "        feat_p = self.fe(pred)    # feature maps\n",
    "        feat_g = self.fe(target)  # feature maps\n",
    "        return self.l1(feat_p, feat_g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7a1913-2a3e-4745-94c9-33b3724ed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel Loss - Bottleneck Diversity\n",
    "\n",
    "class BottleneckDiversityLoss(nn.Module):\n",
    "    def __init__(self, weight=1e-3):\n",
    "        super(BottleneckDiversityLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "    def forward(self, bottleneck_feat):\n",
    "        \"\"\"\n",
    "        bottleneck_feat: [B, C, h, w]\n",
    "        We compute channel-wise covariance and encourage off-diagonal elements to be small.\n",
    "        For efficiency, we approximate by sampling spatial positions or batch.\n",
    "        \"\"\"\n",
    "        B, C, h, w = bottleneck_feat.shape\n",
    "        # Flatten spatial dims: [B, C, h*w]\n",
    "        x = bottleneck_feat.view(B, C, -1)  # [B, C, N]\n",
    "        # Compute per-sample covariance; average over batch\n",
    "        loss = 0.0\n",
    "        for i in range(B):\n",
    "            xi = x[i]  # [C, N]\n",
    "            # zero-mean per channel\n",
    "            xi = xi - xi.mean(dim=1, keepdim=True)\n",
    "            # compute covariance matrix approx: (C x C) but expensive if C large (1024).\n",
    "            # Instead, sample M spatial positions randomly:\n",
    "            N = xi.shape[1]\n",
    "            M = min(100, N)  # sample 100 positions\n",
    "            idx = torch.randperm(N)[:M]\n",
    "            xs = xi[:, idx]  # [C, M]\n",
    "            # compute covariance: cov = xs @ xs.T / (M-1); shape [C,C]\n",
    "            cov = xs @ xs.T / (M - 1.0)\n",
    "            # zero diagonal\n",
    "            cov_off = cov.clone()\n",
    "            cov_off.fill_diagonal_(0)\n",
    "            loss += cov_off.abs().mean()\n",
    "        return self.weight * (loss / B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131ddc70-8f59-4c30-a180-eed7905358e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(pred, target, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    pred: tensor [B,1,H,W], after sigmoid thresholded or use probabilities?\n",
    "    For metric, use binarized prediction at 0.5 threshold or use soft dice?\n",
    "    Here implement soft Dice for evaluation.\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(pred)\n",
    "    B = target.size(0)\n",
    "    dices = []\n",
    "    for i in range(B):\n",
    "        p = probs[i].view(-1)\n",
    "        g = target[i].view(-1)\n",
    "        inter = (p * g).sum()\n",
    "        union = p.sum() + g.sum()\n",
    "        dice = (2 * inter + smooth) / (union + smooth)\n",
    "        dices.append(dice.item())\n",
    "    return np.mean(dices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f021d3-11eb-4e24-a423-b3f786926327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: generic train/eval function (modified for stability and OOM handling)\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, \n",
    "                num_epochs=50, lr=1e-3, weight_decay=1e-5, \n",
    "                scheduler=None, device=torch.device('cuda'), \n",
    "                save_path=None, \n",
    "                hook_bottleneck=False):\n",
    "    \"\"\"\n",
    "    model: nn.Module\n",
    "    train_loader, val_loader: DataLoader\n",
    "    loss_fn: loss function instance; if CombinedLoss with PerceptualLoss, need input_image arg\n",
    "    num_epochs, lr, etc.\n",
    "    hook_bottleneck: if True, register a forward hook on bottleneck to capture features during validation.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if scheduler is None:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    best_val_dice = 0.0\n",
    "\n",
    "    # Hook for bottleneck activations if needed\n",
    "    bottleneck_feats = []\n",
    "    if hook_bottleneck:\n",
    "        def hook_fn(module, inp, outp):\n",
    "            # outp shape [B, C, h, w]; detach and move to CPU immediately\n",
    "            # OPTION A: Store full outp.detach().cpu().clone() — might be large\n",
    "            #    feats = outp.detach().cpu().clone()\n",
    "            #    bottleneck_feats.append(feats)\n",
    "            #\n",
    "            # OPTION B (recommended): store only spatial/channel summary, e.g., global mean per channel:\n",
    "            #    feats_mean = outp.detach().cpu().mean(dim=(0,2,3), keepdim=False).clone()  # shape [C]\n",
    "            #    bottleneck_feats.append(feats_mean)\n",
    "            #\n",
    "            # OPTION C: store small random subset of spatial locations per channel\n",
    "            #    B, C, h, w = outp.shape\n",
    "            #    arr = outp.detach().cpu()\n",
    "            #    # Randomly sample e.g. 100 pixels per channel over the batch:\n",
    "            #    num_samples = min(100, h*w*B)\n",
    "            #    flat = arr.view(-1, C)  # [B*h*w, C]\n",
    "            #    idx = torch.randperm(flat.size(0))[:num_samples]\n",
    "            #    sample = flat[idx].clone()  # [num_samples, C]\n",
    "            #    bottleneck_feats.append(sample)\n",
    "            #\n",
    "            # Choose one option. Here we demonstrate OPTION B to greatly reduce memory:\n",
    "            feats_mean = outp.detach().cpu().mean(dim=(0,2,3), keepdim=False).clone()  # [C]\n",
    "            # if len(bottleneck_feats<20):\n",
    "                # bottleneck_feats.append(feats_mean)\n",
    "\n",
    "        # Register hook on the bottleneck layer of model\n",
    "        # For UNet2D: assume attribute is model.bottleneck; for other: adjust accordingly\n",
    "        handle = model.bottleneck.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        handle = None\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_dices = []\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
    "            images = images.to(device, non_blocking=True)  # [B, C, H, W]\n",
    "            masks  = masks.to(device, non_blocking=True)   # [B, 1, H, W]\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                logits = model(images)\n",
    "                # Some loss_fns need input_image, e.g. PerceptualLoss\n",
    "                if isinstance(loss_fn, PerceptualLoss):\n",
    "                    # modify input_image slice selection as needed\n",
    "                    loss = loss_fn(logits, masks, input_image=images[:,0:1,...])\n",
    "                else:\n",
    "                    loss = loss_fn(logits, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "                with torch.no_grad():\n",
    "                    dice = dice_coeff(logits, masks)\n",
    "                    train_dices.append(dice)\n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    print(f\"WARNING: OOM on batch, skipping batch. Clearing cache. Error: {e}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # optionally: break the loop or continue\n",
    "                    continue\n",
    "                else:\n",
    "                    # re-raise if other error\n",
    "                    raise\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses) if train_losses else float('nan')\n",
    "        avg_train_dice = np.mean(train_dices) if train_dices else float('nan')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_dices = []\n",
    "        # Clear bottleneck_feats before validation\n",
    "        if hook_bottleneck:\n",
    "            bottleneck_feats.clear()\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks  = masks.to(device, non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    logits = model(images)\n",
    "                    if isinstance(loss_fn, PerceptualLoss):\n",
    "                        loss = loss_fn(logits, masks, input_image=images[:,0:1,...])\n",
    "                    else:\n",
    "                        loss = loss_fn(logits, masks)\n",
    "                    val_losses.append(loss.item())\n",
    "                    dice = dice_coeff(logits, masks)\n",
    "                    val_dices.append(dice)\n",
    "                except RuntimeError as e:\n",
    "                    if 'out of memory' in str(e):\n",
    "                        print(f\"WARNING: OOM during validation batch, skipping. Error: {e}\")\n",
    "                        torch.cuda.empty_cache()\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "        avg_val_loss = np.mean(val_losses) if val_losses else float('nan')\n",
    "        avg_val_dice = np.mean(val_dices) if val_dices else float('nan')\n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Train Dice={avg_train_dice:.4f} | Val Loss={avg_val_loss:.4f}, Val Dice={avg_val_dice:.4f}\")\n",
    "\n",
    "        # Scheduler step\n",
    "        # Some schedulers expect a metric; ReduceLROnPlateau uses avg_val_loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save best\n",
    "        if save_path is not None and not np.isnan(avg_val_dice) and avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model at epoch {epoch}, Val Dice={avg_val_dice:.4f}\")\n",
    "        \n",
    "        # After each epoch, free any cached GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally: after each epoch or at checkpoints, analyze bottleneck_feats captured for val set\n",
    "        if hook_bottleneck:\n",
    "            # bottleneck_feats: list of tensors, each shape [C] if OPTION B used\n",
    "            # You can store these per-epoch or process them here:\n",
    "            # For example, stack means: epoch_bott_means = torch.stack(bottleneck_feats)  # [num_batches, C]\n",
    "            pass\n",
    "\n",
    "    # Remove hook handle if registered\n",
    "    if handle is not None:\n",
    "        handle.remove()\n",
    "\n",
    "    return model, best_val_dice, bottleneck_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93a4d1c-a936-42cd-9117-3c332a1e5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def analyze_bottleneck_spectrum(bottleneck_feats, num_components=50):\n",
    "    \"\"\"\n",
    "    bottleneck_feats: list of 1D tensors [C] collected over validation batches.\n",
    "    We stack them into shape [N, C], compute covariance eigenvalues.\n",
    "    \"\"\"\n",
    "    if len(bottleneck_feats) == 0:\n",
    "        print(\"No bottleneck features collected.\")\n",
    "        return None\n",
    "\n",
    "    # Stack: shape [num_batches, C]\n",
    "    data = torch.stack(bottleneck_feats, dim=0).numpy()  # [N, C]\n",
    "    # Zero-mean across batches\n",
    "    data_mean = data.mean(axis=0, keepdims=True)\n",
    "    data_centered = data - data_mean  # [N, C]\n",
    "    # Compute covariance: shape [C,C] might be large. But since N ~ num_batches (e.g. <100), covariance unstable.\n",
    "    # Instead, we can compute covariance of the batch-means: gives insight into channel variation across val batches.\n",
    "    cov = np.cov(data_centered, rowvar=False)  # [C, C]\n",
    "    # Eigenvalues\n",
    "    vals, _ = eigh(cov)\n",
    "    vals = np.sort(vals)[::-1]\n",
    "    vals_norm = vals / (vals.sum() + 1e-12)\n",
    "    # Plot top components\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(vals_norm[:num_components], marker='o')\n",
    "    plt.title(\"Normalized eigenvalues of bottleneck covariance (channel-means)\")\n",
    "    plt.xlabel(\"Component index\")\n",
    "    plt.ylabel(\"Normalized eigenvalue\")\n",
    "    plt.show()\n",
    "    return vals_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6cc9b9-4a95-4e18-af72-ecbf89f5be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def analyze_pixel_separation(bottleneck_feats, val_dataset, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Evaluate whether bottleneck embeddings separate tumor vs background.\n",
    "    Approach: randomly sample pixel embeddings from bottleneck_feats and corresponding downsampled mask.\n",
    "    val_dataset: dataset to get masks for downsampling; ensure index alignment with bottleneck_feats collection.\n",
    "    NOTE: This requires storing which slices correspond to which features; simpler: run on a small subset manually.\n",
    "    For demonstration, do for a single slice:\n",
    "    \"\"\"\n",
    "    # For simplicity, assume we'll analyze one slice: manually pick patient, slice index:\n",
    "    # E.g., from validation: patient_dir, z; run model forward to get bottleneck_feat [1,C,h,w] and mask_slice.\n",
    "    # Then run:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc103110-bc45-4c38-8ae2-af0ccb3f73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_importance(bottleneck_feat, mask_down):\n",
    "    \"\"\"\n",
    "    bottleneck_feat: [1,C,h,w] tensor on CPU\n",
    "    mask_down: np array [h,w] binary 0/1\n",
    "    \"\"\"\n",
    "    C, h, w = bottleneck_feat.shape[1:]\n",
    "    feat = bottleneck_feat[0].view(C, -1).numpy()  # [C, h*w]\n",
    "    labels = mask_down.reshape(-1)\n",
    "    mean_in = feat[:, labels==1].mean(axis=1)\n",
    "    mean_out = feat[:, labels==0].mean(axis=1)\n",
    "    diff = mean_in - mean_out\n",
    "    idx_sorted = np.argsort(-np.abs(diff))\n",
    "    # Return top channels\n",
    "    return idx_sorted, diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1996408e-64f4-47da-8cac-6357d6d7cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Experiments Begin Here On Out-----------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3368b8a1-a332-4db6-be9d-eefbb0727a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTSSliceDataset: 17178 slices (from 258 patients), modalities=['flair']\n",
      "BraTSSliceDataset: 17205 slices (from 111 patients), modalities=['flair']\n",
      "\n",
      "=== Experiment 1: U-Net single-modality FLAIR with loss BCE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|███████████████████| 2148/2148 [12:10<00:00,  2.94it/s]\n",
      "Epoch 1/30 [Val]: 100%|█████████████████████| 2151/2151 [06:05<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0857, Train Dice=0.1350 | Val Loss=0.0397, Val Dice=0.0794\n",
      "Saved best model at epoch 1, Val Dice=0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|███████████████████| 2148/2148 [12:04<00:00,  2.96it/s]\n",
      "Epoch 2/30 [Val]: 100%|█████████████████████| 2151/2151 [05:58<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0817, Train Dice=0.1475 | Val Loss=0.0362, Val Dice=0.0972\n",
      "Saved best model at epoch 2, Val Dice=0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|███████████████████| 2148/2148 [11:56<00:00,  3.00it/s]\n",
      "Epoch 3/30 [Val]: 100%|█████████████████████| 2151/2151 [06:05<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0809, Train Dice=0.1518 | Val Loss=0.0382, Val Dice=0.1078\n",
      "Saved best model at epoch 3, Val Dice=0.1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|███████████████████| 2148/2148 [11:57<00:00,  2.99it/s]\n",
      "Epoch 4/30 [Val]: 100%|█████████████████████| 2151/2151 [06:25<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0805, Train Dice=0.1534 | Val Loss=0.0353, Val Dice=0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|███████████████████| 2148/2148 [11:54<00:00,  3.01it/s]\n",
      "Epoch 5/30 [Val]: 100%|█████████████████████| 2151/2151 [06:04<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0803, Train Dice=0.1539 | Val Loss=0.0325, Val Dice=0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|███████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 6/30 [Val]: 100%|█████████████████████| 2151/2151 [05:58<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0800, Train Dice=0.1555 | Val Loss=0.0330, Val Dice=0.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|███████████████████| 2148/2148 [11:55<00:00,  3.00it/s]\n",
      "Epoch 7/30 [Val]: 100%|█████████████████████| 2151/2151 [06:03<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.0790, Train Dice=0.1613 | Val Loss=0.0314, Val Dice=0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|███████████████████| 2148/2148 [11:54<00:00,  3.01it/s]\n",
      "Epoch 8/30 [Val]: 100%|█████████████████████| 2151/2151 [05:54<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.0802, Train Dice=0.1550 | Val Loss=0.0340, Val Dice=0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|███████████████████| 2148/2148 [11:48<00:00,  3.03it/s]\n",
      "Epoch 9/30 [Val]: 100%|█████████████████████| 2151/2151 [05:52<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.0798, Train Dice=0.1555 | Val Loss=0.0323, Val Dice=0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████████████| 2148/2148 [11:47<00:00,  3.04it/s]\n",
      "Epoch 10/30 [Val]: 100%|████████████████████| 2151/2151 [05:52<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.0797, Train Dice=0.1569 | Val Loss=0.0313, Val Dice=0.1116\n",
      "Saved best model at epoch 10, Val Dice=0.1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 11/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.0797, Train Dice=0.1576 | Val Loss=0.0374, Val Dice=0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 12/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.0792, Train Dice=0.1600 | Val Loss=0.0342, Val Dice=0.1082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 13/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.0793, Train Dice=0.1594 | Val Loss=0.0317, Val Dice=0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 14/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.0793, Train Dice=0.1594 | Val Loss=0.0321, Val Dice=0.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 15/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.0794, Train Dice=0.1596 | Val Loss=0.0342, Val Dice=0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 16/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.0797, Train Dice=0.1564 | Val Loss=0.0358, Val Dice=0.1123\n",
      "Saved best model at epoch 16, Val Dice=0.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 17/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=0.0790, Train Dice=0.1596 | Val Loss=0.0334, Val Dice=0.1142\n",
      "Saved best model at epoch 17, Val Dice=0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████████████| 2148/2148 [11:41<00:00,  3.06it/s]\n",
      "Epoch 18/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.0792, Train Dice=0.1592 | Val Loss=0.0328, Val Dice=0.1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 19/30 [Val]: 100%|████████████████████| 2151/2151 [05:44<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=0.0789, Train Dice=0.1615 | Val Loss=0.0326, Val Dice=0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████████████| 2148/2148 [11:41<00:00,  3.06it/s]\n",
      "Epoch 20/30 [Val]: 100%|████████████████████| 2151/2151 [05:43<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.0791, Train Dice=0.1609 | Val Loss=0.0303, Val Dice=0.1177\n",
      "Saved best model at epoch 20, Val Dice=0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████████████| 2148/2148 [11:42<00:00,  3.06it/s]\n",
      "Epoch 21/30 [Val]: 100%|████████████████████| 2151/2151 [05:44<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss=0.0789, Train Dice=0.1618 | Val Loss=0.0314, Val Dice=0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 22/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss=0.0790, Train Dice=0.1600 | Val Loss=0.0326, Val Dice=0.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 23/30 [Val]: 100%|████████████████████| 2151/2151 [05:47<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss=0.0794, Train Dice=0.1587 | Val Loss=0.0355, Val Dice=0.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.07it/s]\n",
      "Epoch 24/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss=0.0791, Train Dice=0.1604 | Val Loss=0.0333, Val Dice=0.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████████████| 2148/2148 [11:41<00:00,  3.06it/s]\n",
      "Epoch 25/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss=0.0785, Train Dice=0.1631 | Val Loss=0.0332, Val Dice=0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████████████| 2148/2148 [11:40<00:00,  3.06it/s]\n",
      "Epoch 26/30 [Val]: 100%|████████████████████| 2151/2151 [05:44<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss=0.0785, Train Dice=0.1637 | Val Loss=0.0314, Val Dice=0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 27/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss=0.0782, Train Dice=0.1650 | Val Loss=0.0336, Val Dice=0.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 28/30 [Val]: 100%|████████████████████| 2151/2151 [05:46<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss=0.0784, Train Dice=0.1656 | Val Loss=0.0319, Val Dice=0.1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 29/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss=0.0780, Train Dice=0.1671 | Val Loss=0.0314, Val Dice=0.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████████████| 2148/2148 [11:39<00:00,  3.07it/s]\n",
      "Epoch 30/30 [Val]: 100%|████████████████████| 2151/2151 [05:44<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss=0.0783, Train Dice=0.1654 | Val Loss=0.0340, Val Dice=0.1119\n",
      "Loss BCE: Best Val Dice = 0.1177\n",
      "No bottleneck features collected.\n",
      "\n",
      "=== Experiment 1: U-Net single-modality FLAIR with loss Dice ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|███████████████████| 2148/2148 [12:00<00:00,  2.98it/s]\n",
      "Epoch 1/30 [Val]: 100%|█████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.9991, Train Dice=0.0009 | Val Loss=0.4229, Val Dice=0.5771\n",
      "Saved best model at epoch 1, Val Dice=0.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|███████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 2/30 [Val]: 100%|█████████████████████| 2151/2151 [05:47<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.9989, Train Dice=0.0011 | Val Loss=0.4210, Val Dice=0.5790\n",
      "Saved best model at epoch 2, Val Dice=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|███████████████████| 2148/2148 [11:48<00:00,  3.03it/s]\n",
      "Epoch 3/30 [Val]: 100%|█████████████████████| 2151/2151 [05:46<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=0.4210, Val Dice=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|███████████████████| 2148/2148 [11:47<00:00,  3.04it/s]\n",
      "Epoch 4/30 [Val]: 100%|█████████████████████| 2151/2151 [05:47<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.9991, Train Dice=0.0009 | Val Loss=0.4210, Val Dice=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|███████████████████| 2148/2148 [11:50<00:00,  3.03it/s]\n",
      "Epoch 5/30 [Val]: 100%|█████████████████████| 2151/2151 [05:47<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=0.8999, Val Dice=0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|███████████████████| 2148/2148 [11:47<00:00,  3.03it/s]\n",
      "Epoch 6/30 [Val]: 100%|█████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.9986, Train Dice=0.0014 | Val Loss=0.4210, Val Dice=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|███████████████████| 2148/2148 [11:49<00:00,  3.03it/s]\n",
      "Epoch 7/30 [Val]: 100%|█████████████████████| 2151/2151 [05:47<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.9950, Train Dice=0.0050 | Val Loss=0.9787, Val Dice=0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|███████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 8/30 [Val]: 100%|█████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.9955, Train Dice=0.0045 | Val Loss=0.4210, Val Dice=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|███████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 9/30 [Val]: 100%|█████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=0.9133, Val Dice=0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 10/30 [Val]: 100%|████████████████████| 2151/2151 [05:47<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.9962, Train Dice=0.0038 | Val Loss=0.9998, Val Dice=0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 11/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.9855, Train Dice=0.0145 | Val Loss=0.9406, Val Dice=0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.02it/s]\n",
      "Epoch 12/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=1.0000, Val Dice=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.02it/s]\n",
      "Epoch 13/30 [Val]: 100%|████████████████████| 2151/2151 [05:51<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.9998, Train Dice=0.0002 | Val Loss=1.0000, Val Dice=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 14/30 [Val]: 100%|████████████████████| 2151/2151 [05:44<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.9989, Train Dice=0.0011 | Val Loss=0.7450, Val Dice=0.2550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 15/30 [Val]: 100%|████████████████████| 2151/2151 [05:50<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=1.0000, Val Dice=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████████████| 2148/2148 [11:49<00:00,  3.03it/s]\n",
      "Epoch 16/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.9982, Train Dice=0.0018 | Val Loss=0.9794, Val Dice=0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 17/30 [Val]: 100%|████████████████████| 2151/2151 [05:47<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=1.0000, Train Dice=0.0000 | Val Loss=0.9998, Val Dice=0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 18/30 [Val]: 100%|████████████████████| 2151/2151 [05:49<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.7899, Train Dice=0.2101 | Val Loss=0.6962, Val Dice=0.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.01it/s]\n",
      "Epoch 19/30 [Val]: 100%|████████████████████| 2151/2151 [05:47<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=0.6937, Train Dice=0.3063 | Val Loss=0.7527, Val Dice=0.2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.01it/s]\n",
      "Epoch 20/30 [Val]: 100%|████████████████████| 2151/2151 [05:49<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.6877, Train Dice=0.3123 | Val Loss=0.7496, Val Dice=0.2504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 21/30 [Val]: 100%|████████████████████| 2151/2151 [05:50<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss=0.6850, Train Dice=0.3150 | Val Loss=0.7489, Val Dice=0.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.02it/s]\n",
      "Epoch 22/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss=0.6827, Train Dice=0.3173 | Val Loss=0.7444, Val Dice=0.2556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 23/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss=0.6834, Train Dice=0.3166 | Val Loss=0.7464, Val Dice=0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 24/30 [Val]: 100%|████████████████████| 2151/2151 [05:45<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss=0.6776, Train Dice=0.3224 | Val Loss=0.7471, Val Dice=0.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 25/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss=0.6741, Train Dice=0.3259 | Val Loss=0.7421, Val Dice=0.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.01it/s]\n",
      "Epoch 26/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss=0.6795, Train Dice=0.3205 | Val Loss=0.7412, Val Dice=0.2588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████████████| 2148/2148 [11:51<00:00,  3.02it/s]\n",
      "Epoch 27/30 [Val]: 100%|████████████████████| 2151/2151 [05:49<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss=0.6825, Train Dice=0.3175 | Val Loss=0.7411, Val Dice=0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.01it/s]\n",
      "Epoch 28/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss=0.6743, Train Dice=0.3257 | Val Loss=0.7416, Val Dice=0.2584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████████████| 2148/2148 [11:52<00:00,  3.01it/s]\n",
      "Epoch 29/30 [Val]: 100%|████████████████████| 2151/2151 [05:49<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss=0.6778, Train Dice=0.3222 | Val Loss=0.7414, Val Dice=0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████████████| 2148/2148 [11:50<00:00,  3.02it/s]\n",
      "Epoch 30/30 [Val]: 100%|████████████████████| 2151/2151 [05:48<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss=0.6820, Train Dice=0.3180 | Val Loss=0.7400, Val Dice=0.2600\n",
      "Loss Dice: Best Val Dice = 0.5790\n",
      "No bottleneck features collected.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Experiment 1 setup   \n",
    "# Baseline Single-Modality U-Net with Standard Losses\n",
    "modalities = ['flair']\n",
    "train_ds = BraTSSliceDataset(\n",
    "    brats_dir,\n",
    "    patient_list=train_subjects,\n",
    "    modality_mode=modalities,\n",
    "    filter_empty=True,\n",
    "    transforms=random_flip_rotate\n",
    ")\n",
    "val_ds   = BraTSSliceDataset(\n",
    "    brats_dir,\n",
    "    patient_list=val_subjects,\n",
    "    modality_mode=modalities,\n",
    "    filter_empty=False,\n",
    "    transforms=None\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "loss_fns = {\n",
    "    'BCE': nn.BCEWithLogitsLoss(),\n",
    "    'Dice': DiceLoss(),\n",
    "}\n",
    "\n",
    "results_exp1 = {}\n",
    "for name, loss_fn in loss_fns.items():\n",
    "    print(f\"\\n=== Experiment 1: U-Net single-modality FLAIR with loss {name} ===\")\n",
    "    model = UNet2D(in_channels=1, out_classes=1)\n",
    "    save_path = f\"unet_flair_{name}.pth\"\n",
    "    trained_model, best_dice, bott_feats = train_model(\n",
    "        model, train_loader, val_loader, loss_fn,\n",
    "        num_epochs=30, lr=1e-3, device=device,\n",
    "        save_path=save_path, hook_bottleneck=True\n",
    "    )\n",
    "    print(f\"Loss {name}: Best Val Dice = {best_dice:.4f}\")\n",
    "    # Analyze bottleneck representation\n",
    "    vals_norm = analyze_bottleneck_spectrum(bott_feats, num_components=30)\n",
    "    results_exp1[name] = {'best_dice': best_dice, 'eigvals': vals_norm}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be356c-cebe-4aa5-8a83-5089c9b90bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
